# HubLR

i am working on the 42 project called ft_linear_regression, and i do not understand the sense of this subject. We do the hypothesis of a linear model as y = mx + p. We have a dataset, so due to courses from statistics we have formula to calculate m and p directly. So why do compute a gradient descent? The gradient descent is very interesting to code, but in that case, it is unusefull. If the goal of the subject is to code a gradient descent algo, the title should be gradient_descent and not ft_linear_regression, right?


Excellent question â€” and youâ€™re 100% right to be confused at first glance.
Letâ€™s unpack whatâ€™s going on with ft_linear_regression and why youâ€™re being asked to do gradient descent when the â€œsimpleâ€ formulas already exist.

ğŸ§  The math: yes, we can compute m and p directly

For a simple linear regression y=mx+p, you can indeed compute:

![alt text](image.png)

Those formulas come from setting the derivative of the cost function to zero and solving analytically â€” itâ€™s the closed-form solution of the least squares problem.

So if the only goal were to fit a straight line to data, that would be the most efficient way.

âš™ï¸ The reason 42 makes you code gradient descent

The real pedagogical goal of the project isnâ€™t to fit a line â€” itâ€™s to make you understand machine learning optimization.

Machine learning in the real world almost never allows for such simple closed-form solutions.
In more complex models (e.g., logistic regression, neural networks, etc.), you canâ€™t solve the cost function analytically.
You must approximate the minimum of the cost function numerically â€” and gradient descent is the fundamental way to do that.

So, the point of ft_linear_regression is:

Learn how gradient descent works by applying it to the simplest possible case (a line).

Itâ€™s a â€œsandboxâ€ for optimization concepts â€” not a stats exercise.

ğŸ’¡ Why the title â€œft_linear_regressionâ€ still makes sense

Itâ€™s a bit misleading, yes, but the reasoning is:

You are implementing linear regression, conceptually.

The method used to find the coefficients (gradient descent) is whatâ€™s being trained.

The focus is on learning how to optimize parameters given a cost function (the MSE).

So, â€œft_linear_regressionâ€ is the application, and â€œgradient descentâ€ is the technique you use.

ğŸ§© Analogy

Imagine 42 told you to â€œbuild a car,â€ but the real goal was to make you understand how an engine works.
You wouldnâ€™t just buy an engine â€” youâ€™d build one from scratch to understand its moving parts.
Same here: gradient descent is the â€œengineâ€ of most machine learning algorithms.

ğŸ§¾ TL;DR
Question	Answer
Why not use the direct formula?	Because the goal is to understand optimization, not just fit a line.
Is gradient descent mathematically necessary here?	No, itâ€™s overkill â€” but itâ€™s educational.
Why is it called ft_linear_regression then?	Because youâ€™re training a linear model, even though the focus is the optimization method.
So what am I really learning?	How to minimize a cost function via gradient descent â€” the core idea behind all ML.